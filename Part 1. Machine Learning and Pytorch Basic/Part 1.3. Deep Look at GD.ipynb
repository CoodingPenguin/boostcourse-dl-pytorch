{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1.3. Deep Look at GD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ISyXmzPIZvv",
        "colab_type": "text"
      },
      "source": [
        "# Part 1.3. Deep Look at GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNT_YnlxKOTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbVZGo8lIevb",
        "colab_type": "text"
      },
      "source": [
        "## 1. Gradient Descent\n",
        "cost function이 모델의 성능을 나타내는 지표이다. 해당 지표가 작을수록 즉, 에러가 작을수록 모델의 성능은 좋다고 할 수 있다. 그럼 이것을 가지고 어떻게 매개변수를 업데이트 할까?\n",
        "\n",
        "바로 cost function에 대한 각 매개변수의 기울기를 가지고 매개변수를 업데이트한다. 왜냐하면 해당 기울기는 cost function이 작아지는 쪽 방향을 가리키기 때문이다.\n",
        "\n",
        "$$cost(W) = \\frac{1}{m}\\sum_{i=1}^m(Wx^i-y^i)^2$$\n",
        "$$\\triangledown W=\\frac{\\partial cost}{\\partial W} = \\frac{2}{m}\\sum_{i=1}^m(Wx^i-y^i)x^i$$\n",
        "$$W := W - \\alpha\\triangledown W$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmV6JKFvH9ia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "42bb54d3-a16f-49c8-b9d2-430e8e5b75e1"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])\n",
        "\n",
        "# 매개변수 초기화\n",
        "W = torch.zeros(1)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "lr = 0.1\n",
        "nb_epochs = 10\n",
        "\n",
        "# 학습\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = x_train * W\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "  gradient = torch.sum((W*x_train - y_train) * x_train)\n",
        "\n",
        "  print(\"Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}\".format(epoch, nb_epochs, W.item(), cost.item()))\n",
        "\n",
        "  # 매개변수 업데이트\n",
        "  W -= lr*gradient"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
            "Epoch    1/10 W: 1.400, Cost: 0.746666\n",
            "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
            "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
            "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
            "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
            "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
            "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
            "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
            "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
            "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9rO8LpsK2nw",
        "colab_type": "text"
      },
      "source": [
        "## 2. Pytorch Gradient Descent\n",
        "Pytorch에서는 경사하강법 알고리즘을 `optim`이라는 라이브러리에서 제공하고 있다. 꼭 다음과 같은 순서로 경사하강법을 해야 한다.\n",
        "* `optimizer` 정의\n",
        "* `optimizer.zero_grad()`로 기울기 0으로 초기화\n",
        "* `cost.backward()`로 기울기 계산\n",
        "* `optimizer.step()`로 매개변수 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trzd9JILKac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "c73a2d01-4f68-43d4-90cf-381dccd47ec4"
      },
      "source": [
        "# 데이터\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])\n",
        "\n",
        "# 매개변수 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 정의\n",
        "optimizer = torch.optim.SGD([W], lr=0.15)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "nb_epochs = 10\n",
        "\n",
        "# 학습\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = x_train * W\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "  gradient = torch.sum((W*x_train - y_train) * x_train)\n",
        "\n",
        "  print(\"Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}\".format(epoch, nb_epochs, W.item(), cost.item()))\n",
        "\n",
        "  # 매개변수 업데이트\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
            "Epoch    1/10 W: 1.400, Cost: 0.746667\n",
            "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
            "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
            "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
            "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
            "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
            "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
            "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
            "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
            "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}