{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1.4. Multivariable Linear Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ91naqsMRKm",
        "colab_type": "text"
      },
      "source": [
        "# Part 1.4. Multivariable Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9mB2xdkN2SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S836otZxNUnp",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data Definition\n",
        "`Quiz 1`, `Quiz 2`, `Quiz 3`에 따른 내 최종 점수를 예측해보자. 이 때 입력은 `Quiz 1`, `Quiz 2`, `Quiz 3` 3가지이며, 출력은 점수이다.\n",
        "\n",
        "![4-1.png](./img/4-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jex1zqSMMBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdKOzTbtN0Pv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Hypothesis\n",
        "$H(x) = Wx + b$로 나타낼 수 있다. 이 때 $W$는 $\\begin{bmatrix}w_1 & w_2 & w_3\\end{bmatrix}$인 행렬이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mx6Tro3OQNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "eb901053-8ee3-45e8-a05c-573075a88858"
      },
      "source": [
        "W = torch.zeros((3, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "hypothesis = x_train.matmul(W) + b\n",
        "print(hypothesis)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuLsnIwDOpDS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwfKN2OkOvtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "8e9da616-f61b-4473-af8f-c6cd63d650d4"
      },
      "source": [
        "# optimizer 정의\n",
        "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "nb_epochs = 20\n",
        "\n",
        "# 학습\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"Epoch {:4d}/{} hypothesis: {} cost: {:.6f}\".format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) cost: 9298.520508\n",
            "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) cost: 2915.712402\n",
            "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) cost: 915.040527\n",
            "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) cost: 287.936096\n",
            "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) cost: 91.371071\n",
            "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) cost: 29.758249\n",
            "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) cost: 10.445267\n",
            "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) cost: 4.391237\n",
            "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) cost: 2.493121\n",
            "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) cost: 1.897688\n",
            "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) cost: 1.710552\n",
            "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) cost: 1.651416\n",
            "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) cost: 1.632369\n",
            "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) cost: 1.625924\n",
            "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) cost: 1.623420\n",
            "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) cost: 1.622152\n",
            "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) cost: 1.621261\n",
            "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) cost: 1.620501\n",
            "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) cost: 1.619757\n",
            "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) cost: 1.619046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPpqKyWYPiWP",
        "colab_type": "text"
      },
      "source": [
        "## 4. 더 간편하게!\n",
        "`nn.Module`을 상속하여 모델을 생성하면 편하게 신경망을 생성할 수 있다.\n",
        "* `nn.Linear(m, n)` : 선형 모델로 입력이 `m`, 출력이 `n`\n",
        "* `forward()` : hypothesis를 계산\n",
        "* `backward()` : 기울기를 계산\n",
        "\n",
        "또한 `torch.nn.functional`에서는 여러가지 cost function을 제공한다. 자기가 원하는 cost function 함수를 호출하면 굳이 수식을 코드로 적지 않아도 된다.\n",
        "* `F.mse_loss(pred, y)`, `F.l1_loss(pred, y)`, `F.smooth_l1_loss(pred, y)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvA9YS2YQpx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6UdGInVQQtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "f65ab0db-0582-48b6-b77c-ea39c2c0f17a"
      },
      "source": [
        "# 모델 정의 및 초기화\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "\n",
        "# 하이퍼 파라미터 설정\n",
        "nb_epochs = 20\n",
        "\n",
        "# 학습\n",
        "for epoch in range(nb_epochs+1):\n",
        "  hypothesis = model(x_train)\n",
        "  cost = F.mse_loss(hypothesis, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(\"Epoch {:4d}/{} hypothesis: {} cost: {:.6f}\".format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 hypothesis: tensor([-55.8703, -70.9245, -67.8984, -74.2329, -54.8300]) cost: 56385.769531\n",
            "Epoch    1/20 hypothesis: tensor([36.8581, 40.5300, 41.9187, 45.3553, 30.1821]) cost: 17678.546875\n",
            "Epoch    2/20 hypothesis: tensor([ 88.7730, 102.9296, 103.4011, 112.3082,  77.7777]) cost: 5545.882812\n",
            "Epoch    3/20 hypothesis: tensor([117.8378, 137.8651, 137.8228, 149.7926, 104.4251]) cost: 1742.933838\n",
            "Epoch    4/20 hypothesis: tensor([134.1096, 157.4245, 157.0940, 170.7786, 119.3444]) cost: 550.910034\n",
            "Epoch    5/20 hypothesis: tensor([143.2192, 168.3754, 167.8832, 182.5278, 127.6976]) cost: 177.270966\n",
            "Epoch    6/20 hypothesis: tensor([148.3189, 174.5067, 173.9235, 189.1057, 132.3746]) cost: 60.152580\n",
            "Epoch    7/20 hypothesis: tensor([151.1736, 177.9397, 177.3051, 192.7882, 134.9935]) cost: 23.439976\n",
            "Epoch    8/20 hypothesis: tensor([152.7714, 179.8620, 179.1983, 194.8499, 136.4601]) cost: 11.930044\n",
            "Epoch    9/20 hypothesis: tensor([153.6656, 180.9385, 180.2580, 196.0040, 137.2816]) cost: 8.319954\n",
            "Epoch   10/20 hypothesis: tensor([154.1657, 181.5415, 180.8512, 196.6501, 137.7419]) cost: 7.186000\n",
            "Epoch   11/20 hypothesis: tensor([154.4453, 181.8794, 181.1832, 197.0117, 138.0000]) cost: 6.828189\n",
            "Epoch   12/20 hypothesis: tensor([154.6014, 182.0689, 181.3689, 197.2140, 138.1449]) cost: 6.713693\n",
            "Epoch   13/20 hypothesis: tensor([154.6884, 182.1753, 181.4727, 197.3272, 138.2264]) cost: 6.675418\n",
            "Epoch   14/20 hypothesis: tensor([154.7367, 182.2351, 181.5308, 197.3905, 138.2724]) cost: 6.661054\n",
            "Epoch   15/20 hypothesis: tensor([154.7632, 182.2689, 181.5631, 197.4258, 138.2986]) cost: 6.654193\n",
            "Epoch   16/20 hypothesis: tensor([154.7777, 182.2881, 181.5811, 197.4455, 138.3137]) cost: 6.649667\n",
            "Epoch   17/20 hypothesis: tensor([154.7854, 182.2992, 181.5910, 197.4564, 138.3225]) cost: 6.645915\n",
            "Epoch   18/20 hypothesis: tensor([154.7893, 182.3057, 181.5965, 197.4624, 138.3278]) cost: 6.642358\n",
            "Epoch   19/20 hypothesis: tensor([154.7910, 182.3096, 181.5994, 197.4657, 138.3312]) cost: 6.638909\n",
            "Epoch   20/20 hypothesis: tensor([154.7915, 182.3121, 181.6009, 197.4674, 138.3335]) cost: 6.635457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHzKEVD8RPva",
        "colab_type": "text"
      },
      "source": [
        "## 5. Minibatch Gradient Descent\n",
        "데이터 개수가 10만개, 1억개 이렇게 많아지게 되면 한 번에 학습시키는 것이 버거울 수 있다. 계산도 느리고 그 계산량을 감당할 컴퓨터도 없다. 그래서 전체 데이터를 균일하게 나눠서 작은 단위로 한습시키는 **미니배치 학습**을 시킨다.   \n",
        "\n",
        "![4-2.png](./img/4-2.png)\n",
        "\n",
        "업데이트를 좀 더 빠르게 할 수 있다는 장점이 있지만 전체 데이터를 가지고 학습을 하지 않기에 잘못된 방향으로 업데이트가 될 수 있다.\n",
        "\n",
        "![4-3.png](./img/4-3.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r8T4WOoTpsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]                \n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  # 데이터셋의 총 데이터수\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  # 어떤 idx를 받았을 때 그에 상응하는 입출력 데이터를 반환\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "# 데이터셋 생성\n",
        "dataset = CustomDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUBdNTvLUK16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# batch_size는 미니배치의 크기로 보통 2의 제곱수로 설정\n",
        "# shuffle은 각 epoch마다 데이터셋을 섞어서 학습 순서를 바꾸는 것\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnceYAxhVBcp",
        "colab_type": "text"
      },
      "source": [
        "다음은 데이터를 3분할로 나누어 각각을 학습시키는 과정이다. `epoch` 20에, `batch_size` 3으로 따지자면 총 60번 학습을 하게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Hsi2DBUX-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca27e213-9314-4732-8095-e47adfdbf68c"
      },
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        " \n",
        "    prediction = model(x_train)\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # 매개변수 업데이트\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # len(dataloader)는 한 epoch당 미니 매치의 개수\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Batch 1/3 Cost: 4.890475\n",
            "Epoch    0/20 Batch 2/3 Cost: 7.519703\n",
            "Epoch    0/20 Batch 3/3 Cost: 10.077447\n",
            "Epoch    1/20 Batch 1/3 Cost: 2.296371\n",
            "Epoch    1/20 Batch 2/3 Cost: 19.811586\n",
            "Epoch    1/20 Batch 3/3 Cost: 4.386920\n",
            "Epoch    2/20 Batch 1/3 Cost: 8.838188\n",
            "Epoch    2/20 Batch 2/3 Cost: 7.477368\n",
            "Epoch    2/20 Batch 3/3 Cost: 2.407289\n",
            "Epoch    3/20 Batch 1/3 Cost: 6.112224\n",
            "Epoch    3/20 Batch 2/3 Cost: 8.051439\n",
            "Epoch    3/20 Batch 3/3 Cost: 8.984534\n",
            "Epoch    4/20 Batch 1/3 Cost: 1.997134\n",
            "Epoch    4/20 Batch 2/3 Cost: 8.750154\n",
            "Epoch    4/20 Batch 3/3 Cost: 16.186945\n",
            "Epoch    5/20 Batch 1/3 Cost: 10.016115\n",
            "Epoch    5/20 Batch 2/3 Cost: 8.552842\n",
            "Epoch    5/20 Batch 3/3 Cost: 6.859098\n",
            "Epoch    6/20 Batch 1/3 Cost: 11.930544\n",
            "Epoch    6/20 Batch 2/3 Cost: 11.542886\n",
            "Epoch    6/20 Batch 3/3 Cost: 7.841043\n",
            "Epoch    7/20 Batch 1/3 Cost: 8.123621\n",
            "Epoch    7/20 Batch 2/3 Cost: 5.737539\n",
            "Epoch    7/20 Batch 3/3 Cost: 12.446891\n",
            "Epoch    8/20 Batch 1/3 Cost: 4.518116\n",
            "Epoch    8/20 Batch 2/3 Cost: 8.181471\n",
            "Epoch    8/20 Batch 3/3 Cost: 16.565704\n",
            "Epoch    9/20 Batch 1/3 Cost: 6.483998\n",
            "Epoch    9/20 Batch 2/3 Cost: 7.572998\n",
            "Epoch    9/20 Batch 3/3 Cost: 13.745814\n",
            "Epoch   10/20 Batch 1/3 Cost: 8.336905\n",
            "Epoch   10/20 Batch 2/3 Cost: 10.505058\n",
            "Epoch   10/20 Batch 3/3 Cost: 7.049505\n",
            "Epoch   11/20 Batch 1/3 Cost: 11.665935\n",
            "Epoch   11/20 Batch 2/3 Cost: 5.133541\n",
            "Epoch   11/20 Batch 3/3 Cost: 12.366594\n",
            "Epoch   12/20 Batch 1/3 Cost: 12.653246\n",
            "Epoch   12/20 Batch 2/3 Cost: 11.008871\n",
            "Epoch   12/20 Batch 3/3 Cost: 8.012970\n",
            "Epoch   13/20 Batch 1/3 Cost: 8.012122\n",
            "Epoch   13/20 Batch 2/3 Cost: 4.403625\n",
            "Epoch   13/20 Batch 3/3 Cost: 9.352289\n",
            "Epoch   14/20 Batch 1/3 Cost: 2.238928\n",
            "Epoch   14/20 Batch 2/3 Cost: 11.327347\n",
            "Epoch   14/20 Batch 3/3 Cost: 10.215385\n",
            "Epoch   15/20 Batch 1/3 Cost: 4.629120\n",
            "Epoch   15/20 Batch 2/3 Cost: 10.313665\n",
            "Epoch   15/20 Batch 3/3 Cost: 4.905465\n",
            "Epoch   16/20 Batch 1/3 Cost: 8.505123\n",
            "Epoch   16/20 Batch 2/3 Cost: 4.547793\n",
            "Epoch   16/20 Batch 3/3 Cost: 8.840314\n",
            "Epoch   17/20 Batch 1/3 Cost: 2.090931\n",
            "Epoch   17/20 Batch 2/3 Cost: 11.403778\n",
            "Epoch   17/20 Batch 3/3 Cost: 10.291803\n",
            "Epoch   18/20 Batch 1/3 Cost: 10.452560\n",
            "Epoch   18/20 Batch 2/3 Cost: 4.398093\n",
            "Epoch   18/20 Batch 3/3 Cost: 4.970023\n",
            "Epoch   19/20 Batch 1/3 Cost: 12.563957\n",
            "Epoch   19/20 Batch 2/3 Cost: 10.010160\n",
            "Epoch   19/20 Batch 3/3 Cost: 1.469734\n",
            "Epoch   20/20 Batch 1/3 Cost: 9.652735\n",
            "Epoch   20/20 Batch 2/3 Cost: 3.718562\n",
            "Epoch   20/20 Batch 3/3 Cost: 14.132396\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}