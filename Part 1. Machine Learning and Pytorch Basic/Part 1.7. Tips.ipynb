{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1.7. Tips.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM0cCbBr3Yxj",
        "colab_type": "text"
      },
      "source": [
        "# Part 1.7. Tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0cWEWf3irn",
        "colab_type": "text"
      },
      "source": [
        "## 1. Maximum Likelihood Estimation\n",
        "모집단의 분포 함수의 대략적인 형태는 알지만 어떤 정보를 모른다고 할 때, 해당 분포함수를 다음과 같이 나타날 수 있다. 이 때 $\\theta$를 내가 모르는 어떤 정보라 하자.\n",
        "\n",
        "$$f(x;\\theta)$$\n",
        "\n",
        "샘플 $x_1, x_2, ..., x_n$을 얻었다고 할 때 이 샘플의 분포함수는 다음과 같이 나타날 수 있으며 이를 **가능도 함수**라고 한다. 이 샘플를 얻은 것은 우연이 아니라 필연이라고 생각한다.\n",
        "\n",
        "$$L(\\theta;x_1, ... ,x_n) = \\prod_{i=1}^nf(x_i;\\theta)$$\n",
        "\n",
        "이 때 가능도 함수가 최대이도록 하는 $\\theta$를 우리가 원하는 추청값이다.\n",
        "\n",
        "$$L(\\hat{\\theta}^{MLE};x_1, ..., x_n) = \\max_\\theta L(\\theta;x)$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFhFXFqy3mOx",
        "colab_type": "text"
      },
      "source": [
        "## 2. Optimization via Gradient Descent\n",
        "그래서 우리는 이 $\\theta$를 경사하강법(Gradient Descent)를 통해서 찾게 된다. 정확히 말하면 위의 가능도 함수에 대한 $\\theta$의 기울기를 구하여 가능도 함수가 최대가 되도록 $\\theta$를 업데이트 시킨다.\n",
        "\n",
        "$$\\theta \\leftarrow \\theta - \\alpha \\triangledown_\\theta L(x;\\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN7MN3Xe3p9y",
        "colab_type": "text"
      },
      "source": [
        "## 3. Overfitting and Regularization\n",
        "### 3.1. Overfitting\n",
        "오버피팅이란 주어진 데이터에 대해서 과도하게 맞추어졌을 때, 피팅이 되었을 때의 상황을 말한다. MLE의 경우 해당 데이터를 가장 잘 설명하는 확률 분포 함수를 찾다 보니 오버피팅이 일어날 수 밖에 없다.\n",
        "\n",
        "![7-1.png](./img/7-1.png)\n",
        "\n",
        "### 3.2. How to Prevent Overfitting\n",
        "#### 3.2.1. Training, Validation, Test Dataset\n",
        "이 오버피팅을 막기 위해 데이터셋은 **훈련 데이터셋(Train Dataset)**, **검증 데이터셋(Validation Dataset)**, **테스트 데이터셋(Test Dataset)**으로 나누게 된다. 테스트 데이터셋은 훈련 데이터셋과 비슷할 것이라 가정한다. 이 테스트 데이터셋을 통해서 해당 모델의 성능을 평가할 수 있다.\n",
        "\n",
        "더 나아가 테스트 데이터셋도 오버피팅이 되는 경우가 있는데 그걸 방지하기 위해 검증 데이터셋을 테스트 데이터 전에 한 번 검증을 하는 것이다. 이로 모델의 성능을 더 높일 수 있다.\n",
        "\n",
        "![7-2.png](./img/7-2.png)\n",
        "\n",
        "#### 3.2.2. Regularization\n",
        "* **early stopping** : validation loss가 더이상 낮아지지 않을 때 학습을 멈추는 것\n",
        "* **reducing network size** : 신경망의 사이즈를 줄여 학습할 수 있는 양을 줄이는 방법\n",
        "* **weight decay** : 신경망에 쓰이는 매개변수의 개수를 제한\n",
        "* **dropout**과 **batch normalization**이 가장 많이 쓰인다.\n",
        "\n",
        "#### 3.2.3. Other Ways\n",
        "더 많은 데이터를 모으거나 쓸모 없는 피처를 없애는 방법도 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k55GPxefISoX",
        "colab_type": "text"
      },
      "source": [
        "## 4. Basic Approach to Train DNN\n",
        "1. 신경망 구조를 설계한다.\n",
        "2. 학습을 하고 모델이 오버피팅이 되었는지 체크한다.\n",
        "  * 그렇지 않다면 모델을 더 깊고 넓게 확장한다.\n",
        "  * 그렇다면 drop out이나 batch normalization과 같은 regularization을 한다.\n",
        "3. 2.를 반복한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNytUpZHNj8L",
        "colab_type": "text"
      },
      "source": [
        "## 5. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isqpuwwc3SIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ff282e8-80f5-4af0-eee3-ad48de0d7a77"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5ff83a8190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppCW56ayOE1B",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Training and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxDKIXt3ODpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.FloatTensor([[1, 2, 1],                     # (m, 3)\n",
        "                             [1, 3, 2],\n",
        "                             [1, 3, 4],\n",
        "                             [1, 5, 5],\n",
        "                             [1, 7, 5],\n",
        "                             [1, 2, 5],\n",
        "                             [1, 6, 6],\n",
        "                             [1, 7, 7]\n",
        "                            ])\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])        # (m, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Zf9jhXOIZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])   # (m', 3)\n",
        "y_test = torch.LongTensor([2, 2, 2])                            # (m', )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9kcVF-UOXpz",
        "colab_type": "text"
      },
      "source": [
        "### 5.2. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuLIs1HcOZR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 3)   # 피처 3개, 라벨 3개\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgs5saNgOncZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SoftmaxClassifierModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypffb_kIOqSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMxxvPFSO16q",
        "colab_type": "text"
      },
      "source": [
        "### 5.3. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NoPiMOOtbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, x_train, y_train):\n",
        "  nb_epochs = 20\n",
        "  for epoch in range(nb_epochs):\n",
        "    prediction = model(x_train)\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Epoch {:4d}/{} Cost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYel6e3WPfBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, optimizer, x_test, y_test):\n",
        "    prediction = model(x_test)\n",
        "    predicted_classes = prediction.max(1)[1]    # (최대값, 최대값의 인덱스)\n",
        "\n",
        "    correct_count = (predicted_classes == y_test).sum().item()\n",
        "    cost = F.cross_entropy(prediction, y_test)\n",
        "\n",
        "    print('Accuracy: {}% Cost: {:.6f}'.format(correct_count / len(y_test) * 100, cost.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTNxd-xhPr-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1d6dc730-abea-4155-a8c4-005005423e8a"
      },
      "source": [
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 2.203667\n",
            "Epoch    1/20 Cost: 1.199645\n",
            "Epoch    2/20 Cost: 1.142985\n",
            "Epoch    3/20 Cost: 1.117769\n",
            "Epoch    4/20 Cost: 1.100901\n",
            "Epoch    5/20 Cost: 1.089523\n",
            "Epoch    6/20 Cost: 1.079872\n",
            "Epoch    7/20 Cost: 1.071320\n",
            "Epoch    8/20 Cost: 1.063325\n",
            "Epoch    9/20 Cost: 1.055720\n",
            "Epoch   10/20 Cost: 1.048378\n",
            "Epoch   11/20 Cost: 1.041245\n",
            "Epoch   12/20 Cost: 1.034285\n",
            "Epoch   13/20 Cost: 1.027478\n",
            "Epoch   14/20 Cost: 1.020813\n",
            "Epoch   15/20 Cost: 1.014279\n",
            "Epoch   16/20 Cost: 1.007872\n",
            "Epoch   17/20 Cost: 1.001586\n",
            "Epoch   18/20 Cost: 0.995419\n",
            "Epoch   19/20 Cost: 0.989365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXUWBFLqP1y6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4f943b6-040d-4c3a-f6ef-dbc8eca27c58"
      },
      "source": [
        "test(model, optimizer, x_test, y_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0% Cost: 1.425844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeI9Trq8QEcq",
        "colab_type": "text"
      },
      "source": [
        "## 6. Learning Rate\n",
        "학습 속도를 조절할 수 있는 하이퍼파라미터이다. 학습률이 너무 크면 발산하면서 cost가 점점 늘어날 수 있다. 반대로 학습률이 너무 작으면 cost가 너무 줄어들지 않는다. 그러니 발산하면 줄이고, 줄어들지 않으면 키우면서 적절한 값을 찾는 것이 중요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L3dnCCnQGGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "a2411bad-a350-4abd-cdc8-868060fab625"
      },
      "source": [
        "# learning rate가 클 때\n",
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e5)\n",
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 3.187324\n",
            "Epoch    1/20 Cost: 1100707.375000\n",
            "Epoch    2/20 Cost: 2482261.000000\n",
            "Epoch    3/20 Cost: 664769.875000\n",
            "Epoch    4/20 Cost: 1668198.875000\n",
            "Epoch    5/20 Cost: 748657.812500\n",
            "Epoch    6/20 Cost: 1353832.375000\n",
            "Epoch    7/20 Cost: 1790073.750000\n",
            "Epoch    8/20 Cost: 917894.875000\n",
            "Epoch    9/20 Cost: 989687.125000\n",
            "Epoch   10/20 Cost: 990845.250000\n",
            "Epoch   11/20 Cost: 1585082.500000\n",
            "Epoch   12/20 Cost: 1265073.750000\n",
            "Epoch   13/20 Cost: 1149145.000000\n",
            "Epoch   14/20 Cost: 589766.937500\n",
            "Epoch   15/20 Cost: 689678.625000\n",
            "Epoch   16/20 Cost: 983032.687500\n",
            "Epoch   17/20 Cost: 1265073.750000\n",
            "Epoch   18/20 Cost: 1686645.000000\n",
            "Epoch   19/20 Cost: 484999.593750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AJR4-dXQvLx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "4e580a09-8849-4c77-a502-1f3a1fb65ae6"
      },
      "source": [
        "# learning rate가 작을 때\n",
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-10)\n",
        "train(model, optimizer, x_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 1.341574\n",
            "Epoch    1/20 Cost: 1.341574\n",
            "Epoch    2/20 Cost: 1.341574\n",
            "Epoch    3/20 Cost: 1.341574\n",
            "Epoch    4/20 Cost: 1.341574\n",
            "Epoch    5/20 Cost: 1.341574\n",
            "Epoch    6/20 Cost: 1.341574\n",
            "Epoch    7/20 Cost: 1.341574\n",
            "Epoch    8/20 Cost: 1.341574\n",
            "Epoch    9/20 Cost: 1.341574\n",
            "Epoch   10/20 Cost: 1.341574\n",
            "Epoch   11/20 Cost: 1.341574\n",
            "Epoch   12/20 Cost: 1.341574\n",
            "Epoch   13/20 Cost: 1.341574\n",
            "Epoch   14/20 Cost: 1.341574\n",
            "Epoch   15/20 Cost: 1.341574\n",
            "Epoch   16/20 Cost: 1.341574\n",
            "Epoch   17/20 Cost: 1.341574\n",
            "Epoch   18/20 Cost: 1.341574\n",
            "Epoch   19/20 Cost: 1.341574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghBWVY2OQ05b",
        "colab_type": "text"
      },
      "source": [
        "## 7. Data Preprocessing\n",
        "머신이 학습하기 쉽도록 데이터를 미리 전처리하는 과정을 말한다. 예를 들어 한 피처의 값의 범위가 1000단위이고 다른 한 범위가 소수점 단위라고 할 때 신경망은 1000단위의 값을 조정하는 게 더 이득을 보기에 해당 피처를 가지고만 학습을 할 것이다. 이를 방지하기 위해 전처리를 해주어 동등하게 학습하도록 해주어야 한다.\n",
        "\n",
        "![7-3.png](./img/7-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeFtB-SmRMfx",
        "colab_type": "text"
      },
      "source": [
        "### 7.1. Standardization\n",
        "데이터의 수치를 정규 분포로 바꿔주는 방법. 이 때 우리가 가진 데이터가 정규 분포를 따른다고 가정하고 평균($\\mu$)과 표준편차($\\sigma$)를 이용해 정규화시켜준다.\n",
        "\n",
        "$$x'_j = \\frac{x_j-\\mu_j}{\\sigma_j}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXxcvepaQyxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMQa3FaaR7ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "fbd44155-0022-40d9-f299-71e317bc4a22"
      },
      "source": [
        "mu = x_train.mean(dim=0)                  # 평균\n",
        "sigma = x_train.std(dim=0)                # 표준편차\n",
        "norm_x_train = (x_train - mu) / sigma     # 정규화된 데이터\n",
        "\n",
        "print(norm_x_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0674, -0.3758, -0.8398],\n",
            "        [ 0.7418,  0.2778,  0.5863],\n",
            "        [ 0.3799,  0.5229,  0.3486],\n",
            "        [ 1.0132,  1.0948,  1.1409],\n",
            "        [-1.0674, -1.5197, -1.2360]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calm0qBtSJs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)   # 3개의 피처로 특정 수치를 예측\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsj-puVtSL8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MultivariateLinearRegressionModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERNT3BZSSNaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqvg256tSOau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, x_train, y_train):\n",
        "    nb_epochs = 20\n",
        "    for epoch in range(nb_epochs):\n",
        "        prediction = model(x_train)\n",
        "        cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiC67E1aSUnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "6265ad6f-3b53-47d3-b307-6e05009b6bf9"
      },
      "source": [
        "train(model, optimizer, norm_x_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Cost: 29785.089844\n",
            "Epoch    1/20 Cost: 18906.164062\n",
            "Epoch    2/20 Cost: 12054.674805\n",
            "Epoch    3/20 Cost: 7702.029785\n",
            "Epoch    4/20 Cost: 4925.733398\n",
            "Epoch    5/20 Cost: 3151.632812\n",
            "Epoch    6/20 Cost: 2016.996094\n",
            "Epoch    7/20 Cost: 1291.051270\n",
            "Epoch    8/20 Cost: 826.505249\n",
            "Epoch    9/20 Cost: 529.207397\n",
            "Epoch   10/20 Cost: 338.934204\n",
            "Epoch   11/20 Cost: 217.153564\n",
            "Epoch   12/20 Cost: 139.206757\n",
            "Epoch   13/20 Cost: 89.313782\n",
            "Epoch   14/20 Cost: 57.375465\n",
            "Epoch   15/20 Cost: 36.928429\n",
            "Epoch   16/20 Cost: 23.835773\n",
            "Epoch   17/20 Cost: 15.450401\n",
            "Epoch   18/20 Cost: 10.077809\n",
            "Epoch   19/20 Cost: 6.633700\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}